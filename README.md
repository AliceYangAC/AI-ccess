# ğŸ§  AI-ccess

**AI-powered accessibility microservice** that adapts user interfaces, interprets voice commands, generates image captions, and personalizes experiences for users with diverse needs.

---

## Features

- **Voice Command Interpretation**  
  Uses transformer-based NLP to classify spoken commands and trigger UI actions like theme switching, layout changes, and navigation.

- **Image Captioning**  
  Generates alt text for uploaded images using deep learning models to support screen readers and improve content accessibility.

---

## ğŸ—ï¸ Project Structure

```
AI-CCESS/
â”œâ”€â”€ flask_frontend/         # Flask-based UI
â”‚   â”œâ”€â”€ app.py              # Frontend logic
â”‚   â”œâ”€â”€ templates/          # HTML templates
â”‚   â””â”€â”€ static/             # CSS/JS assets
â”œâ”€â”€ models/                 # ML models
â”‚   â”œâ”€â”€ caption_model.py    # Image captioning
â”‚   â””â”€â”€ nlp_model.py        # Voice intent classification
â”œâ”€â”€ services/               # Core microservices
â”‚   â”œâ”€â”€ accessibility.py    # UI adaptation logic
â”‚   â”œâ”€â”€ captioning.py       # Caption generation
â”‚   â””â”€â”€ feedback.py         # Feedback handling
â”œâ”€â”€ schemas.py              # Data schemas
â”œâ”€â”€ main.py                 # FastAPI backend
â”œâ”€â”€ requirements.txt        # Dependencies
â””â”€â”€ README.md               # You're reading it!
```

---

## ğŸ› ï¸ Setup Instructions

### 1. Clone the repo

```bash
git clone https://github.com/AliceYangAC/AI-ccess.git
cd AI-ccess
```

### 2. Create a virtual environment

```bash
python -m venv .venv
source .venv\Scripts\activate.ps1 # on Windows
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Run the FastAPI backend

```bash
uvicorn main:app --reload
```

### 5. Run the Flask frontend

```bash
cd flask_frontend
python app.py
```

---

## Demo Pages

- `/` â€“ Landing page
- `/captioning` â€“ Upload images for caption generation
- `/voice` â€“ Voice-controlled dashboard with layout adaptation
- `/ui-adapt` â€“ Manually test UI adaptation logic (WIP)

---

## Powered By

- [Transformers](https://huggingface.co/transformers/) for NLP
- [FastAPI](https://fastapi.tiangolo.com/) for backend services
- [Flask](https://flask.palletsprojects.com/) for frontend
- [Bootstrap](https://getbootstrap.com/) for styling

---

## ğŸ“Œ Future Enhancements

### Priority

- **UI Adaptation Engine**  
  Dynamically adjusts layout, font size, contrast, and navigation mode based on user preferences or detected accessibility needs.

- **Accessibility Detection**  
  Analyzes user settings (e.g., zoom level, screen reader usage) to infer accessibility requirements and adapt the interface accordingly.

- **Feedback Collection**  
  Captures user feedback to refine personalization and improve accessibility recommendations over time.

### Other Ideas

- Fine-tuned intent classifier for accessibility-specific commands
- Cloud-based speech-to-text integration (e.g., Whisper, Azure Speech)
- Multilingual voice support

---

## ğŸ“„ License

MIT License. See `LICENSE` file for details.
